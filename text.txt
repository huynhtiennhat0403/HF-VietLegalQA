### TÃ³m táº¯t ná»™i dung Ä‘Ã£ há»c trong â€œLLM â€“ Hugging Faceâ€

- Transformer vÃ  kiáº¿n trÃºc
    - Ba há» mÃ´ hÃ¬nh: encoder-only (BERT) cho phÃ¢n loáº¡i, NER, QA trÃ­ch xuáº¥t; decoder-only (GPT) cho sinh vÄƒn báº£n, QA táº¡o sinh; encoderâ€“decoder (BART) cho dá»‹ch, sá»­a ngá»¯ phÃ¡p, tÃ³m táº¯t, QA táº¡o sinh.
    - CÆ¡ cháº¿ attention vÃ  tá»‘i Æ°u hÃ³a cho chuá»—i dÃ i: LSH attention (Reformer), local attention (Longformer), axial positional encodings.[[1]](https://www.notion.so/LLM-Hugging-Face-27d385efe63b8081a3b3f027410d9ef7?pvs=21)
- Suy luáº­n (inference) vá»›i LLMs
    - KhÃ¡i niá»‡m context length vÃ  cÃ¡c yáº¿u tá»‘ áº£nh hÆ°á»Ÿng.
    - Quy trÃ¬nh hai pha: Prefill (tokenize â†’ embed â†’ xá»­ lÃ½ ngá»¯ cáº£nh ban Ä‘áº§u) vÃ  Decode (tá»± há»“i quy: attention â†’ phÃ¢n phá»‘i xÃ¡c suáº¥t â†’ chá»n token â†’ dá»«ng).
    - Chiáº¿n lÆ°á»£c sampling: temperature, topâ€‘k/topâ€‘p, pháº¡t láº·p láº¡i (frequency, presence), kiá»ƒm soÃ¡t Ä‘á»™ dÃ i báº±ng giá»›i háº¡n token, stop sequences, EOS.[[2]](https://www.notion.so/LLM-Hugging-Face-27d385efe63b8081a3b3f027410d9ef7?pvs=21)
- Sá»­ dá»¥ng ğŸ¤— Transformers
    - Pipeline vÃ  thÃ nh pháº§n tokenizer: chuyá»ƒn vÄƒn báº£n â†’ token, thÃªm special tokens, Ã¡nh xáº¡ sang ID.
    - LÆ°u/táº£i mÃ´ hÃ¬nh: config.json vÃ  model.safetensors; Ä‘áº©y mÃ´ hÃ¬nh lÃªn Hub báº±ng notebook_login/huggingfaceâ€‘cli vÃ  push_to_hub.
    - Thá»±c hÃ nh tokenizer: padding, truncation, decode/encode vÃ­ dá»¥ vá»›i BERT.[[3]](https://www.notion.so/LLM-Hugging-Face-27d385efe63b8081a3b3f027410d9ef7?pvs=21)
- CÃ¡c kiá»ƒu tokenizer vÃ  so sÃ¡nh
    - Wordâ€‘based, characterâ€‘based, subword (Byteâ€‘level BPE, WordPiece, SentencePiece/Unigram): Æ°u nhÆ°á»£c Ä‘iá»ƒm, vÃ­ dá»¥ minh há»a.
    - Xá»­ lÃ½ batch Ä‘a Ä‘á»™ dÃ i: padding Ä‘Ãºng cÃ¡ch, attention mask Ä‘á»ƒ trÃ¡nh mÃ´ hÃ¬nh â€œhá»câ€ tá»« padding; tráº£ vá» tensor PyTorch/NumPy; cáº¥u hÃ¬nh padding â€œlongestâ€, â€œmax_lengthâ€, truncation.[[4]](https://www.notion.so/LLM-Hugging-Face-27d385efe63b8081a3b3f027410d9ef7?pvs=21)
- ğŸ¤— Datasets: náº¡p, lÃ m sáº¡ch vÃ  biáº¿n Ä‘á»•i dá»¯ liá»‡u
    - Náº¡p local/remote, láº¥y máº«u, kiá»ƒm tra trÃ¹ng láº·p, Ä‘á»•i tÃªn cá»™t.
    - LÃ m sáº¡ch: chuáº©n hÃ³a nhÃ£n, táº¡o Ä‘áº·c trÆ°ng (vÃ­ dá»¥ review_length), lá»c outliers, giáº£i mÃ£ HTML.
    - Map batched Ä‘á»ƒ tÄƒng tá»‘c; sá»‘ liá»‡u benchmark fast vs. slow tokenizer khi batched.[[5]](https://www.notion.so/LLM-Hugging-Face-27d385efe63b8081a3b3f027410d9ef7?pvs=21)
- ğŸ¤— Tokenizers chuyÃªn sÃ¢u
    - Huáº¥n luyá»‡n tokenizer má»›i tá»« tokenizer cÅ© báº±ng AutoTokenizer.train_new_from_iterator, lÆ°u vÃ  chia sáº» lÃªn Hub.
    - Fast tokenizer (Rust) vs. slow (Python): tá»‘c Ä‘á»™ vÃ  tÃ­nh nÄƒng nÃ¢ng cao nhÆ° BatchEncoding, tokens(), word_ids(), offset mapping, sentence_ids().
    - Chuáº©n hÃ³a vÃ  preâ€‘tokenization; tá»•ng quan thuáº­t toÃ¡n subword (BPE, WordPiece, Unigram) vÃ  cÃ¡ch hoáº¡t Ä‘á»™ng khi train vÃ  khi tokenize (vÃ­ dá»¥ xá»­ lÃ½ [UNK], tiá»n tá»‘ ## cá»§a WordPiece).[[6]](https://www.notion.so/LLM-Hugging-Face-27d385efe63b8081a3b3f027410d9ef7?pvs=21)
- Fineâ€‘tuning vÃ  cÃ¡c bÃ i toÃ¡n NLP cá»• Ä‘iá»ƒn
    - Token classification (NER, POS): cÄƒn chá»‰nh nhÃ£n vá»›i subword, gÃ¡n -100 cho special tokens.
    - Domain adaptation vá»›i Masked LM: concat vÃ  chunk dá»¯ liá»‡u, DataCollatorForLanguageModeling, Ä‘Ã¡nh giÃ¡ perplexity.
    - Dá»‹ch mÃ¡y: pipeline, tiá»n xá»­ lÃ½ vá»›i text_target, DataCollatorForSeq2Seq, Ä‘Ã¡nh giÃ¡ SacreBLEU.
    - TÃ³m táº¯t: chá»n táº­p, lá»c dá»¯ liá»‡u, tiá»n xá»­ lÃ½ tokenizer cho tiÃªu Ä‘á» vÃ  ngá»¯ cáº£nh.
    - Huáº¥n luyá»‡n causal LM tá»« Ä‘áº§u vÃ  Q&A: tÃ i liá»‡u, liÃªn káº¿t tham kháº£o trÃªn Hub.[[7]](https://www.notion.so/LLM-Hugging-Face-27d385efe63b8081a3b3f027410d9ef7?pvs=21)
- Káº¿t luáº­n thá»±c hÃ nh chÃ­nh
    - Hiá»ƒu kiáº¿n trÃºc Transformer vÃ  áº£nh hÆ°á»Ÿng cá»§a context length Ä‘áº¿n inference.
    - LÃ m chá»§ tokenizer vÃ  dá»¯ liá»‡u: padding, masks, truncation, batching.
    - Quy trÃ¬nh endâ€‘toâ€‘end vá»›i há»‡ sinh thÃ¡i HF: Datasets â†’ Tokenizers â†’ Transformers â†’ Hub.
    - Fineâ€‘tuning theo bÃ i toÃ¡n vÃ  theo miá»n dá»¯ liá»‡u, Ä‘Ã¡nh giÃ¡ báº±ng thÆ°á»›c Ä‘o phÃ¹ há»£p (perplexity, BLEU, v.v.).[[8]](https://www.notion.so/LLM-Hugging-Face-27d385efe63b8081a3b3f027410d9ef7?pvs=21)

Báº¡n muá»‘n mÃ¬nh rÃºt gá»n hÆ¡n thÃ nh â€œcheat sheetâ€ 1 trang, hay nháº¥n máº¡nh vÃ o pháº§n báº¡n sáº½ Ã¡p dá»¥ng ngay nhÆ° tokenization hay fineâ€‘tuning?